{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LPUUr_q8f4C"
      },
      "source": [
        "Google Colab에서 Google Drive를 마운트해서 드라이브 내 파일들에 접근 허용\n",
        "`with open('경로', 'r') as _ : ` 등으로 파일 읽기 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g5of12mN8mVr"
      },
      "outputs": [],
      "source": [
        "# 계산, 통계에 자주 사용되는 Numpy 라이브러리\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LEOJ3oxx9GHO"
      },
      "outputs": [],
      "source": [
        "# Python 정규표현식 모듈\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y57PsQK9uRG"
      },
      "source": [
        "## 1. 입력데이터 전처리 작업\n",
        "**1.한글, 숫자, 영어를 제외한 글자는 모두 '_'로 치환**\n",
        "\n",
        "[^...] : 부정 문자 클래스, 괄호 안의 문자들이 아닌 것들을 매칭\n",
        "\n",
        "0-9 : 아라비아 숫자 (0~9)\n",
        "\n",
        "a-zA-Z : 영어 알파벳 (소문자 + 대문자)\n",
        "\n",
        "\\u3131-\\u3163 : 한글 자모 범위 (ㄱ ~ ㅣ)\n",
        "\n",
        "\\uac00-\\ud7a3 : 완성형 한글 (가 ~ 힣)\n",
        "\n",
        "**2. 언더바가 하나 이상일 경우 언더바 하나로 치환**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YNGG_wcP9GrM"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    \"\"\"\n",
        "    input_text = '아 이 영화 스토리랑 연기랑;; 개ㅐㅐ노잼^^!!!!! 인데 왜 9점임???'\n",
        "    output_text = '$아_이_영화_스토리랑_연기랑_개ㅐㅐ노잼_인데_왜_9점임_$'\n",
        "    \"\"\"\n",
        "\n",
        "    non_alpha_numeric_hangul = re.compile('[^0-9a-zA-Z\\u3131-\\u3163\\uac00-\\ud7a3]')\n",
        "    sentense_separator = '$'\n",
        "\n",
        "    # text의 대상 문자열을 _ 로 변경\n",
        "    t = non_alpha_numeric_hangul.sub('_', text)\n",
        "\n",
        "    # _가 2개 이상일 경우, 하나로 치환\n",
        "    text = re.sub(r'_+', '_', t)\n",
        "    return f'{sentense_separator}{text}{sentense_separator}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_N0DdDFF5fe"
      },
      "source": [
        "# 2. 입력 파일 형식에 맞춰 data와 target 값 읽어오기\n",
        "---\n",
        "\n",
        "**id\tdocument\tlabel**<br>\n",
        "6270596\t굳 ㅋ\t1 <br>\n",
        "9274899\tGDNTOPCLASSINTHECLUB\t0 <br>\n",
        "8544678\t뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\t0<br>\n",
        "6825595\t지루하지는 않은데 완전 막장임... 돈주고 보기에는....\t0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9jlLvUpz92gq"
      },
      "outputs": [],
      "source": [
        "def prepare_data_file(FILE_PATH):\n",
        "\n",
        "    # 경로로 파일 열기\n",
        "    with open(FILE_PATH) as f:\n",
        "        lines = f.read().split('\\n') # 한 줄씩 읽어오기\n",
        "\n",
        "    data, target = [], []\n",
        "    for l in lines[1:]: # 첫째 줄은 컬럼명이라 읽어오지 않음\n",
        "        try:\n",
        "            _, text, label = l.strip().split('\\t') # 탭으로 구분\n",
        "        except ValueError:\n",
        "            pass\n",
        "        text = text.strip() # text 내부의 공백 제거\n",
        "        if text == '' : continue # 공백 데이터는 학습 데이터로 사용하지 않음\n",
        "        data.append(preprocess_text(text))\n",
        "        target.append(int(label))\n",
        "\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keuKlFCZ32E2"
      },
      "source": [
        "# 3. Bigram feature를 추출하여 feature_dict으로 만들어서 리턴\n",
        "MAX_FEATURES : 사용할 feature 개수\n",
        "\n",
        "_feature_dict는 ML 벡터화에 쓰이는 biagram -> 숫자 맵핑테이블_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Dwa0P8QW38Jg"
      },
      "outputs": [],
      "source": [
        "def extract_features(data, MAX_FEATURES):\n",
        "\n",
        "    FEATURES = dict()\n",
        "\n",
        "    \"\"\"\n",
        "    bigram을 생성하고 등장 빈도수 체크\n",
        "    \"\"\"\n",
        "\n",
        "    for line in data:\n",
        "        uni_list = list(line)\n",
        "        bi_list = [''.join(uni_list[z:z+2]) for z in range(0, len(uni_list)-1)]\n",
        "\n",
        "        for bigram in bi_list:\n",
        "\n",
        "            if bigram in FEATURES: # 빈도수 누적\n",
        "                FEATURES[bigram] += 1\n",
        "            else:\n",
        "                FEATURES[bigram] = 1\n",
        "\n",
        "    features_list = [(b, c) for (b, c) in FEATURES.items()]\n",
        "    features_list.sort(reverse=True, key=lambda z:z[1]) # count 순으로 내림차순 정렬\n",
        "\n",
        "    features_dict = dict()\n",
        "    for (idx, (b, c)) in enumerate(features_list[:MAX_FEATURES]):\n",
        "        features_dict[b] = idx\n",
        "\n",
        "    return features_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPbWiDXaDsjT"
      },
      "source": [
        "# 4. 입력 문장을 고정된 크기의 Feature  Vector로 변환하여 ML 학습에 사용할 입력 벡터 생성\n",
        "Bigram 기반의 이진 벡터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WvCq04Xz8nPw"
      },
      "outputs": [],
      "source": [
        "def make_feature_vector(feature_set, data, target):\n",
        "\n",
        "    # 기본 벡터 템플릿 생성 [0,0,0,0, ..., 0]\n",
        "    fv_base = [0 for _ in range(0, len(feature_set))]\n",
        "    feature_list = []\n",
        "\n",
        "    for (x, label) in zip(data, target):\n",
        "        uni_list = list(x)\n",
        "        fv = fv_base[:]\n",
        "        bi_list = [''.join(uni_list[z:z+2]) for z in range(0, len(uni_list)-1)]\n",
        "\n",
        "        # one-hot 방식\n",
        "        for bigram in bi_list:\n",
        "            if bigram in feature_set:\n",
        "                fv[feature_set[bigram]] = 1\n",
        "        feature_list.append(fv + [label])\n",
        "    feature_list = np.array(feature_list)\n",
        "\n",
        "    # ML시 train/test bias를 방지하기 위해 셔플\n",
        "    np.random.shuffle(feature_list)\n",
        "\n",
        "    return feature_list[:, :-1], feature_list[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV4EuvO2MAIR"
      },
      "source": [
        "## 학습데이터, 평가데이터 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHFgopnZDwoF",
        "outputId": "f2e43cab-b39e-400d-d911-51a0c9b8e70c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prepare_data_file START...\n",
            "prepare_data_file END...\n"
          ]
        }
      ],
      "source": [
        "TRAIN_FILE = 'ratings_train.txt'\n",
        "TEST_FILE  = 'ratings_test.txt'\n",
        "\n",
        "print('prepare_data_file START...')\n",
        "train_data, train_target = prepare_data_file(TRAIN_FILE)\n",
        "test_data, test_target = prepare_data_file(TEST_FILE)\n",
        "print('prepare_data_file END...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9oPdv6vMVEv"
      },
      "source": [
        "## 학습데이터로부터 MAX_FEATURES개의 Bigram Feature 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXDYqR0BMU2R",
        "outputId": "7810fcd0-f117-4839-e133-0792e09b798f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "extract_features START...\n",
            "extract_features END...\n"
          ]
        }
      ],
      "source": [
        "print('extract_features START...')\n",
        "MAX_FEATURES = 1000\n",
        "feature_set = extract_features(train_data, MAX_FEATURES)\n",
        "with open('features.out', 'w', encoding='utf-8') as fo:\n",
        "    fo.write('\\n'.join([x+str(idx) for x, idx in feature_set.items()]))\n",
        "\n",
        "print('extract_features END...')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOs4cUyjNLEy"
      },
      "source": [
        "## 입력 파일을 고정된 크기의 feature vector로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSW0KegPNO5m",
        "outputId": "416916ea-d171-47a7-be11-dd83d9022cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "make_feature_vector START...\n",
            "make_feature_vector END...\n"
          ]
        }
      ],
      "source": [
        "print('make_feature_vector START...')\n",
        "x_train, y_train = make_feature_vector(feature_set, train_data, train_target)\n",
        "x_test, y_test = make_feature_vector(feature_set, test_data, test_target)\n",
        "print('make_feature_vector END...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFlEl1jNPEuo"
      },
      "source": [
        "## ML 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMOTRI0CNpUk",
        "outputId": "080a7bad-a933-4185-9fd3-a8b7a79621e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train START...\n",
            "train END...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "print('train START...')\n",
        "model = MultinomialNB()\n",
        "model.fit(x_train, y_train)\n",
        "print('train END...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EER9iC27PXiQ"
      },
      "source": [
        "## 정확도 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvTvuRM7PVfd",
        "outputId": "7540c907-bdc1-458c-950f-2532ed418ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval START...\n",
            "TRAIN ACCURACY : 0.780614\n",
            "TEST SCORE : 0.777651\n",
            "evel END...\n"
          ]
        }
      ],
      "source": [
        "print('eval START...')\n",
        "print(f'TRAIN ACCURACY : {model.score(x_train, y_train):3f}')\n",
        "print(f'TEST SCORE : {model.score(x_test, y_test):3f}')\n",
        "print('evel END...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqxKNUHQItd"
      },
      "source": [
        "## 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhgBDqRMPr3V",
        "outputId": "6afe9517-845d-4034-88b8-ecc545cbe42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$어쩌다_이_영화를_보게_되었는지_함꼐_본_사람은_좋았지만_영화는_그를_잊을만큼_정말_별로였어요$ ==> Positive\n"
          ]
        }
      ],
      "source": [
        "# 예측 실패\n",
        "text = '어쩌다 이 영화를 보게 되었는지.. 함꼐 본 사람은 좋았지만 영화는 그를 잊을만큼 정말 별로였어요'\n",
        "text = preprocess_text(text)\n",
        "x_test, _ = make_feature_vector(feature_set, [text], [None])\n",
        "result = model.predict(x_test)\n",
        "print(text, '==>', ['Negative','Positive'][result[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj0_JYYhQgyR",
        "outputId": "7e4bfa2a-fdd7-4e82-ac3e-67db388d4bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$연기도_재밌고_존잼_$ ==> Positive\n"
          ]
        }
      ],
      "source": [
        "# 예측 성공\n",
        "text = '연기도 재밌고 존잼!'\n",
        "text = preprocess_text(text)\n",
        "x_test, _ = make_feature_vector(feature_set, [text], [None])\n",
        "result = model.predict(x_test)\n",
        "print(text, '==>', ['Negative','Positive'][result[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOC-xwQWQmTr",
        "outputId": "d0be7eea-2046-4588-a2ee-9feac856df7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "$ㅈㄴ별로임_진짜_하_돈_아까워$ ==> Negative\n"
          ]
        }
      ],
      "source": [
        "# 예측 성공\n",
        "text = 'ㅈㄴ별로임 진짜... 하.... 돈 아까워'\n",
        "text = preprocess_text(text)\n",
        "x_test, _ = make_feature_vector(feature_set, [text], [None])\n",
        "result = model.predict(x_test)\n",
        "print(text, '==>', ['Negative','Positive'][result[0]])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
